:PROPERTIES:
:ID:       6f952799-ab73-473a-af5f-45faa882716e
:END:
#+title: slow-is-smooth-and-smooth-is-fast

In many sports (I first heard it in the context of boxing) there is a saying that 'slow is smooth and smooth is fast'.
It means that to move fast in the long term, you should slow down your movement today and ensure you move smoothly and with good technique.
In my experience, this concept applies outside of sports and outside of individuals.


I tend not to shut up about the importance of bieng rigorous while programming.
If you rush to implement a feature today at the expense of proper automated testing, code readability and structure, you will pay for it later.
Bugs, restructurng, and general confusion will plague the next feature you try to implement and make it take longer.
Maybe you manage to rush that one out as well, only to cause bigger problems going forward.
If you're working in a team this effect is amplified, because instead of just you paying for your sloppiness, now your whole team has to.
I think this is fairly uncontroversial, and if you want some broad strokes on how to avoid these issues, see [[id:d2494e49-8840-4773-8783-6cd586ded217][best-practices-for-programming]].

What may be more controversial is how I think this applies to scientific communities.
I'm simultaneously staggered by how many papers I have to read to keep up with my field and by how long it takes for real progress to take place in our understanding of the brain.
I'm certainly not alone in thinking we need fewer, higher-quality publications.

Even so, in my own research I find myself constantly making compromises in order to get it done: better a flawed paper than none at all, right?
Things take so much longer than you thought possible and in order to have even a chance at a career you have to still get things done within a reasonable timeframe.
So the study you thought was easily feasible in 6 months not only takes a year, but has to have corners cut, important avenues left unexplored, and ultimately its usefulness reduced.

In my case one major delay was that I planned a paper and associated experiments on the basis of two studies that turned out not to be statistically robust.
After several months of work I realized the mistake both these papers shared - a failure to correct for the number of statistical comparisons performed - and found that the empirical basis on which I was modifying my model was extremely shaky.
I opted to drop that direction of research, and I threw away 6 months of work in the process.

Two other major setbacks came from faults in the work of other researchers.
One dataset I relied on for building my model turned out to be entirely inaccurate for the purposes of my research, and so required me to rebuild my model from scratch.
Earlier drafts of the dataset had been better, but there were no automated checks in place to ensure the researchers didn't accidentally make it worse instead of better.
In another case some software I was relying on was not well tested and simply broke in a variety of bewildering ways over the course of several months, necessitating rebuilding my model a few times.
Now I have to cut corners on my own research, introducing known limitations that I otherwise could have addressed, in large part because of these setbacks.
Those I relied on also had to deal with problematic datasets and buggy software in their research, making their jobs harder than they could be.
Like them, I now pay the shittery forward.

# # A lot of the papers I do read have poor statistical power and may have publication bias.
# I'm certainly not the only person who thinks we need fewer, higher quality studies - I think it's a consensus really.

Perhaps the frequent changes and job insecurity typical to academic research have something to do with this.

work in progress.

