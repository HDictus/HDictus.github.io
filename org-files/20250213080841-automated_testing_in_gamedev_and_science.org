:PROPERTIES:
:ID:       75493881-e118-49a3-bf70-403cb76dcc19
:END:
#+title: automated-testing-in-gamedev-and-science

I've noticed that the use of automated testing in computational science is sparse - many researchers don't use it at all, and those who do only do so when releasing a package.
I also have the impression that it is uncommon in game development, I think for similar reasons.

I try to consistently write comprehensive tests when coding, whether doing science for work or gamedev for fun.
I have experience applying testing consistently in both domains.
With embarassing frequency, my discipline slips because of impatience, and I write untested code.
Occaisionally I deliberately eschew automated testing because I feel it is unnecessary.
Rarely, I have even been right about this.

Computational modeling and game development have something in common, in that both involve constructing and simulating an artificial world.
In computational modeling you want this artificial world to accurately reflect some part of the real world.
In game development this is often true as well, but the departures from reality are usually as important, if not more so, than the similarities.

The reasons that automated testing is uncommon in both also comes down to similar reasons.
This is doubly true as data-oriented design, essential to cache efficient performance in HPC, catches on in game development.
I'd like to take you through the development of my upcoming game and the challenges I encountered with automated testing.
I believe these lessons can be applied as well to computaional science.

* Background: Concord ECS

I used an entity-component-system (ECS) package called Concord for the game.[todo link]
In an ECS the game logic is encoded in objects called systems.
Systems should not, in principle, contain state: that is stored in the entities and components.

Concord uses the [[id:67af4334-012d-4cb0-b257-b7be7d911e78][observer-pattern]] to manage these systems.
Each system is an object with a few defined methods, for instance:

```
PhysicsSystem = Concord.system({collidable={'hitbox'}})

function PhysicsSystem:update(dt)
end

function PhysicsSystem:collision(entity, other)
end

```

The system is then subscribed to the game world

```
world = Concord.world()
world.addSystem(PhysicsSystem)
```

And events are subsequently emitted by the world, such that

```
world:emit("update", 0.1)
```
will result in ```PhysicsSystem:update(0.1)``` being called.

Components are where all the world's state is stored.
You might have a position component, and a rotation component, or a `transform` component with `transform.position` and `transform.rotation`.
Components are attached to entities, and this is how objects in the world are resolved: they are nothing more than a cluster of components.

* Early on: brittle unit testing

Since the game logic is contained in the systems, when I started automated testing I initially treated these as units and tested them.
Each system has filters attached, which contain the entities that have particular components.
The systems only operate on the contents of these filters, so by setting the filters, calling the system's methods, and then examining the contents of the filters I was able to unit test their behavior.
However, I quickly ran into problems.

Not only were these tests quite tedious to write: specifying all the components in the filters and mocking interactions between the world and system, but they failed to catch most of the important bugs and were incredibly brittle.
In improving the structure of the code I was constantly moving functionality from one system to another, or splitting it off into new systems, and this always meant rewriting tests even though the behavior of the program as a whole didn't change.
Since all interactions between systems were indirect the specifics of how each individual system worked were also not that essential to keeping the game bug-free.
Over time I ended up changing all my brittle tests to scenario based tests instead.






* Challenge and reward in prototyping

One of the similarities between game development and science is the importance of prototyping.
You frequently want to try stuff to see if it will work.
By testing the general behavior of the game I actually go useful feedback on how a proposed change to the mechanics would interact with the esablished scenarios.

1. the intervening world
   I often found that I wanted my systems to prompt the world to emit events, and call the designated event.
   I now needed to mock this behavior of the world.
   
2. shifting responsibilities

3. addition of components

Two of these are only relevant with this architecture, but 2 would be with any advanced object-oriented system as well.
You should be revising the responsibilities of objects as you get a clearer picture of what needs doing.
This was primarily an issue early on.
Later in the development process, unit testing becomes stronger again.



A challenge that is more strong for computational science is the lack of encapsulation.
Preventing brittleness of testing requires hiding implementation details in testing.
Hiding details about how data is stored and accessed incurs computational costs, which is often prohibitive for computational work.
Computational models often keep representations of data very direct rather than hiding it behind abstractions for this reason.




The first and main reason is changeability : you're constantly trying things out to see if they work: the intended behavior of the program changing constantly rather than the implementation.
This makes testing harder to justify because you may not keep the feature you're implementing anyway.



































* The biggest challenge: change

In games and science you are constantly prototyping.
What if I make this mechanic work like /this/?
What if I model this with a polynomial instead of linear relationship?


* uniqueness

Game development especially is usually a domain with lots of exceptions.
Most games have scripted elements, cases in which something specific has to happen just once.
Similarly in science, in my experience, a lot of your work is in throwaway scripts - programs that transform a specific dataset and spit out a specific output and are (in theory) never used again.

This makes it very tempting to periodically run the program as you build it and manually check that it is working.
If the script genuinely is one-use and independent of other tools you can get away with this.
If you are indeed very careful in your manual testing, checking everything that could be wrong as you go, you can be confident that your program works.
But if you missed something, and have to go back and change things again, how will that affect the other parts of your program?
If the script is sufficiently complicated, it is hard to be sure you won't break as much as you fix.
In that event, having broken down your script into tested functions will save you a great deal of time.
A nice middle ground is to insert assertions in the script itself - checking at each step that assumptions are respected and the values make sense.
Still, unit tests that explicitly check edge cases for the different functions give a better guarantee of correctness.
In practice, I've found that things you think are throwaway scripts rarely are.

You may decide later to rerun it on a different dataset, or rerun it slighly differently.
Moreover, chunks of code you put in your throwaway script are always useful elsewhere, and you save considerable time by reusing them.


Copy-pasting this code is risky.
If you change your mind on what kinds of assumptions you want to make, or what formulae you are using for a given calculation you now have to remember whether and where you pasted this code to.
If you don't, you run the risk of being logically inconsistent in your analysis and possibly harming the validity of your results.
Best is to put it in a module somewhere, test it, and re-use it by importing the module.
You don't have to do this right away - it would be a waste of time, and bloat your modules, to do this with chunks of code you never reuse.
But the unit tests are best written right away, alongside the code itself.
Not only do they help you write clearer code upfront, but it also ensures you write the function in a testable way.
Adding unit tests later is always more work than adding them upfront.

In my game, I have a few scripted sequences that I nonetheless tested.
Each of these sequences relies on game mechanics that were already implemented, game mechanics that are subject to change.
If I decide to modify how they work, and it breaks a scripted sequence, I would like to be notified.
It might show up in

This is messy, it seems I still can't quite put into words wha I'm thinking here.
I guess my argument is that in game development integration/acceptance tests are more important.
The lack of automated testing in game dev is down to the ways in which it is not well suited to unit testing.
and I can make th argument that this is true for scientific computing as well.


One thing to note is that I still have issues with brittleness: when I run the working game, things tests for working functionality are broken
Ah, nevermind: in this case I had changed some functionality without adjusting tests.
A major motivating factor in this kind of thing is wanting to see fast results.
It's a marshmallow test.

But I do think that the changeability of stuff, the constant tweaking is what makes unit testing prohibitive.


https://www.youtube.com/watch?v=EZ05e7EMOLM
and
https://vimeo.com/80533536

Architecture and design already somewhat constrained by engine choice, so you don't need to constrain the design as much?

If I choose to change how a component represents some property, I need to wrap it in the tests beforehand.
In an OOP approach this is already done by encapsulation, and where it isn't it ought to be.
For an ECS, changing how data is laid out / represented is a bigger issue, as data layout is the interface used by systems.
You could isolate this to a single system which is then the only one to directly interact with the data.
But at that point, you're not longer doing DOD, and are less likely to reap its performance benefits.

